{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping each topic's human content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from datetime import datetime, timedelta\n",
    "import trafilatura\n",
    "import re\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of economic topics\n",
    "topics = [\n",
    "    \"economic policy\", \"economy\", \"inflation\", \"interest rates\",\n",
    "    \"recession\", \"employment\", \"unemployment\", \"federal reserve\", \"stock market\"\n",
    "]\n",
    "\n",
    "# Trusted news domains\n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "## --- DATE RANGE CONFIGURATION ---\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 2/25\n",
    "start_date = datetime.strptime(\"2025-02-17\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-03-01\", \"%Y-%m-%d\")\n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8  \n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df  = final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df = pd.concat([econ_df, final_df], ignore_index=True)\n",
    "\n",
    "econ_df = econ_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "econ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "econ_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_econ = econ_df[econ_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_econ[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the human econ data to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_econ.to_csv(\"human_econ.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining summary stats for token usage on the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encode(df):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    df[\"token_count\"] = df[\"content\"].apply(lambda x: len(encoding.encode(str(x))))\n",
    "\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Political"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Political topics\n",
    "topics = [\n",
    "    \"presidential election\", \"voter turnout\", \"election fraud\",\n",
    "    \"ballot access\", \"voting rights\", \"political campaigns\",\n",
    "    \"political debates\", \"swing states\", \"presidential inauguration\", \"Congress\", \"policy agenda\", \"executive orders\"\n",
    "]\n",
    "\n",
    "# Trusted news domains  \n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 2/25\n",
    "start_date = datetime.strptime(\"2025-02-26\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-03-01\", \"%Y-%m-%d\") \n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_pol_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_pol_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_df = final_pol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_df = pd.concat([pol_df, final_pol_df], ignore_index=True)\n",
    "\n",
    "pol_df = pol_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "pol_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_pol = pol_df[pol_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the pol df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol.to_csv(\"human_pol.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining summary stats for token usage on the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encode(df):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    df[\"token_count\"] = df[\"content\"].apply(lambda x: len(encoding.encode(str(x))))\n",
    "\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of conflict topics\n",
    "topics = [\n",
    "    \"global conflict\", \"Ukraine war\", \"Israel Hamas\", \"military escalation\", \"foreign policy\", \n",
    "    \"diplomatic tensions\", \"international sanctions\", \"tariffs\", \"trade war\", \"United Nations\"\n",
    "\n",
    "]\n",
    "\n",
    "# Trusted news domains  \n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 2/25\n",
    "start_date = datetime.strptime(\"2025-02-15\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-03-01\", \"%Y-%m-%d\")\n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_con_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_con_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = final_con_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = pd.concat([con_df, final_con_df], ignore_index=True)\n",
    "\n",
    "con_df = con_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "con_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_con = con_df[con_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the conflict df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_con.to_csv(\"human_con.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining summary stats for token usage on the article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encode(df):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    df[\"token_count\"] = df[\"content\"].apply(lambda x: len(encoding.encode(str(x))))\n",
    "\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of AI topics\n",
    "topics = [\n",
    "    \"artificial intelligence\", \"AI regulation\", \"machine learning\", \"AI ethics\", \"deepfakes\", \"data privacy\", \"social media moderation\", \"algorithmic bias\", \"Large Language Models\",\n",
    "    \"Chat-GPT\", \"Deepseek\"\n",
    "]\n",
    "\n",
    "\n",
    "# Trusted news domains  \n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 3/25\n",
    "start_date = datetime.strptime(\"2024-10-15\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-11-01\", \"%Y-%m-%d\") \n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8\n",
    "\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_ai_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_ai_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df = final_ai_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df = pd.concat([ai_df, final_ai_df], ignore_index=True)\n",
    "\n",
    "ai_df = ai_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "ai_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the resulting topic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_ai = ai_df[ai_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examines the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the AI df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai.to_csv(\"human_ai.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the previous tokenizing summary statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Climate topics\n",
    "topics = [\n",
    "    \"climate change\", \"climate policy\", \"global warming\", \"carbon emissions\", \"green energy\", \"climate crisis\", \"extreme weather\", \"renewable energy\",\n",
    "    \"natural disaster\"\n",
    "]\n",
    "\n",
    "\n",
    "# Trusted news domains  \n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 2/25\n",
    "start_date = datetime.strptime(\"2025-02-15\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-03-01\", \"%Y-%m-%d\")  \n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8 \n",
    "\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_climate_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_climate_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df = final_climate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df = pd.concat([climate_df, final_climate_df], ignore_index=True)\n",
    "\n",
    "climate_df = climate_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "climate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the resulting topic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_climate = climate_df[climate_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the climate df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_climate.to_csv(\"human_climate.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the previous tokenizing summary statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Health topics\n",
    "topics = [\n",
    "     \"Cancer research\", \"medical breakthroughs\", \"Obesity\", \"Vaccine development\", \"Infectious diseases\", \n",
    "     \"Mental health crisis\", \"Health disparities\", \"opiod epidemic\"\n",
    "]\n",
    "\n",
    "# Trusted news domains  \n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 9/24 through 3/25\n",
    "start_date = datetime.strptime(\"2024-09-01\", \"%Y-%m-%d\") \n",
    "end_date = datetime.strptime(\"2024-09-10\", \"%Y-%m-%d\") \n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_health_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_health_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df = final_health_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df = pd.concat([health_df, final_health_df], ignore_index=True)\n",
    "\n",
    "health_df = health_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "health_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the resulting topic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_health = health_df[health_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health[\"title\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the Health df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health.to_csv(\"human_health.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the previous tokenizing summary statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Law topics\n",
    "topics = [\n",
    "  \"Supreme Court ruling\", \"abortion ban\", \"student debt decision\", \"judicial review\", \"constitutional law\", \"civil rights cases\", \"Roe V. Wade\", \"Constitutional Ammendment\",\n",
    "]\n",
    "\n",
    "# Trusted news domains\n",
    "trusted_sources = [\n",
    "    \"bbc.com\", \"reuters.com\", \"cnn.com\", \"theguardian.com\", \"techcrunch.com\",\n",
    "    \"theverge.com\", \"foxnews.com\", \"npr.org\", \"apnews.com\", \"aljazeera.com\",\n",
    "    \"politico.com\", \"axios.com\", \"cbsnews.com\", \"abcnews.go.com\"\n",
    "]\n",
    "\n",
    "# Set date range and manually change after each run\n",
    "# Date range for this topic was 10/24 through 3/25\n",
    "start_date = datetime.strptime(\"2025-02-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2025-02-15\", \"%Y-%m-%d\") \n",
    "# Seconds between each request to avoid rate limiting\n",
    "sleep_time = 8 \n",
    "\n",
    "\n",
    "# Generate daily date ranges from start_date to end_date\n",
    "def generate_daily_ranges(start, end):\n",
    "    ranges = []\n",
    "    current = start\n",
    "    while current < end:\n",
    "        next_day = current + timedelta(days=1)\n",
    "        ranges.append((current.strftime(\"%Y-%m-%d\"), next_day.strftime(\"%Y-%m-%d\")))\n",
    "        current = next_day\n",
    "    return ranges\n",
    "\n",
    "# Extract the full article from the url using trafilatura\n",
    "def get_article_text(url):\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            return trafilatura.extract(downloaded)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# --- MAIN SCRAPER ---\n",
    "\n",
    "# Initialize the GDELT wrapper\n",
    "gd = GdeltDoc()\n",
    "all_articles = []\n",
    "date_ranges = generate_daily_ranges(start_date, end_date)\n",
    "\n",
    "for topic in topics:\n",
    "    for start, end in date_ranges:\n",
    "        retries = 0\n",
    "        success = False\n",
    "        # Adds error handling\n",
    "        while not success and retries < 3:\n",
    "            try:\n",
    "                filters = Filters(keyword=topic, start_date=start, end_date=end)\n",
    "                articles = gd.article_search(filters)\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "                if not articles.empty:\n",
    "                    articles[\"topic\"] = topic\n",
    "                    all_articles.append(articles)\n",
    "                success = True\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Retry {retries}: Failed {topic} from {start} to {end} - {e}\")\n",
    "                time.sleep(sleep_time * 2)\n",
    "\n",
    "# --- FILTER AND SAVE RESULTS ---\n",
    "\n",
    "if all_articles:\n",
    "    # Combines all the results into a single df\n",
    "    combined = pd.concat(all_articles, ignore_index=True)\n",
    "    # Filter for only english articles, from the sources, and a long enough title\n",
    "    filtered = combined[\n",
    "        (combined[\"language\"] == \"English\") &\n",
    "        (combined[\"domain\"].isin(trusted_sources)) &\n",
    "        (combined[\"title\"].str.len() > 30)\n",
    "    ].copy()\n",
    "\n",
    "    # Applying function to get full article content\n",
    "    filtered[\"content\"] = filtered[\"url\"].apply(get_article_text)\n",
    "    final_law_df = filtered.dropna(subset=[\"content\"])[[\"title\", \"url\", \"content\", \"topic\"]]\n",
    "else:\n",
    "    final_law_df = pd.DataFrame(columns=[\"title\", \"url\", \"content\", \"topic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs only after the first time the code above is ran to make concatenation possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_df = final_law_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concats the articles from the different date ranges together. Block needs to be run each time the article generation code is run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_df = pd.concat([law_df, final_law_df], ignore_index=True)\n",
    "\n",
    "law_df = law_df.drop_duplicates(subset=[\"title\"])\n",
    "\n",
    "law_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examines the resulting topic value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_df[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code filters out unsuitable articles based on specific characterisitics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_suitable(row):\n",
    "    title = str(row['title']).strip()\n",
    "    content = str(row['content']).strip()\n",
    "\n",
    "    # if the title has too few words\n",
    "    if len(title.split()) < 5:\n",
    "        return False\n",
    "    # if title is a question\n",
    "    if '?' in title:\n",
    "        return False\n",
    "    # if title indicates that the article content isn't journalistic\n",
    "    if title.lower().startswith(('opinion', 'analysis', 'explainer', 'live', 'editorial', 'fact check', 'why ', 'how ', 'what ', 'who ')):\n",
    "        return False\n",
    "    # if article content is too short\n",
    "    if len(content.split()) < 100:\n",
    "        return False  \n",
    "    # if the article has too few sentences\n",
    "    if content.count('.') < 3:\n",
    "        return False\n",
    "    # if articles starts with a number\n",
    "    if re.match(r'^\\d+\\s', title):  \n",
    "        return False\n",
    "    # if title contains non-journalistic phrases\n",
    "    if any(phrase in title.lower() for phrase in ['this week', 'meet the press', 'face the nation', 'moderated by', 'featuring', \"everything to know\", \"what you need to know\", \"pros and cons\",\n",
    "    \"how to make the most of\", \"top trends in\", \"step-by-step guide\", \"ways to\", \"reasons to\", \"review\", \"word of the year\", \"transcript\", \"photo collection\", \"briefing\"]):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "df_law = law_df[law_df.apply(is_suitable, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examines the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_law[\"content\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the Law df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_law.to_csv(\"human_law.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the previous tokenizing summary statistics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode(df_law)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the AI text based on human title content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Economic AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=850,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure gpt_text column exists\n",
    "if \"gpt_text\" not in df_econ.columns:\n",
    "    df_econ[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_econ.iterrows(), total=len(df_econ)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_econ.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_econ.to_csv(\"econ_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1)\n",
    "\n",
    "# Final save to a csv\n",
    "df_econ.to_csv(\"econ_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'econ_gpt_text.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the first and second article that was generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_econ[\"gpt_text\"].dropna().iloc[0]\n",
    "print(len(first_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_econ[\"gpt_text\"].dropna().iloc[1]\n",
    "# print(first_article)\n",
    "print(len(first_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the token summarization function but for the gpt generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encode_gpt(df):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    df[\"token_count_ai\"] = df[\"gpt_text\"].apply(lambda x: len(encoding.encode(str(x))))\n",
    "\n",
    "    return df.describe()\n",
    "\n",
    "token_encode_gpt(df_econ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Political AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_pol.columns:\n",
    "    df_pol[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_pol.iterrows(), total=len(df_pol)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_pol.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_pol.to_csv(\"pol_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1)\n",
    "\n",
    "# Final save to a csv\n",
    "df_pol.to_csv(\"pol_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'pol_gpt_text.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine first article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_pol[\"gpt_text\"].dropna().iloc[0]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Conflicts AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=950,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_con.columns:\n",
    "    df_con[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_con.iterrows(), total=len(df_con)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_con.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_con.to_csv(\"conflicts_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1)\n",
    "\n",
    "# Final save to a csv\n",
    "df_con.to_csv(\"conflicts_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'conflicts_gpt_text.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the first articles generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_con[\"gpt_text\"].dropna().iloc[0]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ai = pd.read_csv(\"human_ai.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artificial Intelligence AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_ai.columns:\n",
    "    df_ai[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_ai.iterrows(), total=len(df_ai)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_ai.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_ai.to_csv(\"ai_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1)\n",
    "\n",
    "# Final save to a csv\n",
    "df_ai.to_csv(\"ai_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'ai_gpt_text.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the first article generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_ai[\"gpt_text\"].dropna().iloc[0]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Climate AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1050,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_climate.columns:\n",
    "    df_climate[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_climate.iterrows(), total=len(df_climate)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_climate.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_climate.to_csv(\"climate_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1) \n",
    "\n",
    "# Final save to a csv\n",
    "df_climate.to_csv(\"climate_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'climate_gpt_text.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the first article generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_climate[\"gpt_text\"].dropna().iloc[0]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_climate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Health AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1050,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_health.columns:\n",
    "    df_health[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_health.iterrows(), total=len(df_health)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_health.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_health.to_csv(\"health_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1) \n",
    "\n",
    "# Final save to a csv\n",
    "df_health.to_csv(\"health_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'health_gpt_text.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the first article generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_article = df_health[\"gpt_text\"].dropna().iloc[0]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Law AI Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"your personal api key\")\n",
    "\n",
    "# Define function that'll prompt ChatGPT-4o to write the article\n",
    "def generate_gpt_article(headline):\n",
    "    prompt = (\n",
    "        f\"You are a journalist writing a full news article based only on the headline below. \"\n",
    "        f\"The event occurred after April 2024, and you have no access to real-world information about what actually happened. \"\n",
    "        f\"Do not include a byline or dateline  just write the full article text. \"\n",
    "        f\"Headline: \\\"{headline}\\\"\\n\\n\"\n",
    "        f\"Article:\"\n",
    "    )\n",
    "    # Specifying parameters and returning the written article\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=1100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error on headline: {headline[:60]}...  {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Make sure 'gpt_text' column exists\n",
    "if \"gpt_text\" not in df_law.columns:\n",
    "    df_law[\"gpt_text\"] = None\n",
    "\n",
    "# Loop to generate GPT articles\n",
    "# tdqm provides a status bar\n",
    "for idx, row in tqdm(df_law.iterrows(), total=len(df_law)):\n",
    "    if pd.isna(row[\"gpt_text\"]) and pd.notna(row[\"title\"]):\n",
    "        gpt_article = generate_gpt_article(row[\"title\"])\n",
    "        df_law.at[idx, \"gpt_text\"] = gpt_article\n",
    "\n",
    "        # Save progress every 50 rows\n",
    "        if idx % 50 == 0:\n",
    "            df_law.to_csv(\"law_with_gpt_text_partial.csv\", index=False)\n",
    "\n",
    "        time.sleep(1.1)\n",
    "\n",
    "# Final save to a csv\n",
    "df_law.to_csv(\"law_gpt_text_final.csv\", index=False)\n",
    "print(\"All GPT articles generated and saved under 'law_gpt_text.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the gpt tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encode_gpt(df_law)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
